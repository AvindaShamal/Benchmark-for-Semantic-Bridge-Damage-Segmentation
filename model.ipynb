{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob as gglob\n",
    "from os.path import join as joiner\n",
    "from os import makedirs\n",
    "from skimage.transform import resize\n",
    "from pathlib import Path \n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "###############################\n",
    "# misc\n",
    "###############################\n",
    "\n",
    "TARGET_LIST = ['Crack', 'ACrack', 'Wetspot', 'Efflorescence', 'Rust', 'Rockpocket', 'Hollowareas', 'Cavity',\n",
    "               'Spalling', 'Graffiti', 'Weathering', 'Restformwork', 'ExposedRebars', \n",
    "               'Bearing', 'EJoint', 'Drainage', 'PEquipment', 'JTape', 'WConccor']\n",
    "\n",
    "def open_json(file):\n",
    "    with open(file) as f:\n",
    "        d = json.load(f)\n",
    "    return d\n",
    "\n",
    "def save_dict(dct, file):\n",
    "    with open(file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dct, f, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "# labelme2mask\n",
    "###############################\n",
    "\n",
    "def labelme2mask(data):\n",
    "    \n",
    "    TARGET_LIST = ['Crack', 'ACrack', 'Wetspot', 'Efflorescence', 'Rust', 'Rockpocket', 'Hollowareas', 'Cavity',\n",
    "               'Spalling', 'Graffiti', 'Weathering', 'Restformwork', 'ExposedRebars', \n",
    "               'Bearing', 'EJoint', 'Drainage', 'PEquipment', 'JTape', 'WConccor']\n",
    "\n",
    "    if isinstance(data, str):\n",
    "        with open(data, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    assert type(data) == dict\n",
    "        \n",
    "    target_dict = dict(zip(TARGET_LIST, range(len(TARGET_LIST))))\n",
    "    height = data[\"size\"][\"height\"]\n",
    "    width = data[\"size\"][\"width\"]\n",
    "    target_mask = np.zeros((height, width, len(TARGET_LIST)))\n",
    "    for obj in data[\"objects\"]:\n",
    "        label = obj[\"classTitle\"]\n",
    "        if label in TARGET_LIST:\n",
    "            # Get the target index (channel) for this label\n",
    "            target_index = target_dict[label]\n",
    "            \n",
    "            # Create an empty mask for the current object\n",
    "            target_img = Image.new('L', (width, height), 0)\n",
    "            \n",
    "            # Get the exterior polygon points from the \"points\" field\n",
    "            polygon = [(x, y) for x, y in obj[\"points\"][\"exterior\"]]  # Convert list to tuple\n",
    "            \n",
    "            # Draw the polygon on the mask\n",
    "            ImageDraw.Draw(target_img).polygon(polygon, outline=1, fill=1)\n",
    "            \n",
    "            # Add the polygon mask to the appropriate channel in the target mask\n",
    "            target_mask[:, :, target_index] += np.array(target_img)               \n",
    "    return target_mask.astype(bool).astype(np.uint8)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize images\n",
    "\n",
    "def resize_images(source_folder, target_folder, size=(512,512)):\n",
    "    \"\"\"Resize all images in `source_folder` to size `size` and store it in `target_folder`\"\"\"\n",
    "    image_files = sorted(gglob(joiner(source_folder, \"*.jpg\")))\n",
    "    makedirs(target_folder, exist_ok = True)\n",
    "    \n",
    "    for image_filename in tqdm(image_files):\n",
    "        base_filename = Path(image_filename).name\n",
    "        target_name = joiner(target_folder, base_filename)\n",
    "        with Image.open(image_filename) as img:\n",
    "            img = img.resize(size)\n",
    "            img = img.save(target_name)\n",
    "    print(f\"Resized images saved in {target_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize annotations\n",
    "\n",
    "def _resize_annotation_data(data, width_factor, height_factor):\n",
    "    w_index, h_index = 0, 1\n",
    "    \n",
    "    # Iterate through all objects\n",
    "    for obj in data[\"objects\"]:\n",
    "        # If the object has an exterior polygon (points)\n",
    "        if \"points\" in obj and \"exterior\" in obj[\"points\"]:\n",
    "            # Get the length of points in the \"exterior\" polygon\n",
    "            point_len = len(obj[\"points\"][\"exterior\"])\n",
    "            \n",
    "            # Loop through each point in the polygon\n",
    "            for point_idx in range(point_len):\n",
    "                the_w = obj[\"points\"][\"exterior\"][point_idx][w_index]\n",
    "                the_h = obj[\"points\"][\"exterior\"][point_idx][h_index]\n",
    "\n",
    "                # Apply the resizing factors\n",
    "                obj[\"points\"][\"exterior\"][point_idx][w_index] = the_w / width_factor\n",
    "                obj[\"points\"][\"exterior\"][point_idx][h_index] = the_h / height_factor\n",
    "                \n",
    "    return data\n",
    "\n",
    "def resize_annotations(source_folder, target_folder, size=(512,512)):\n",
    "    \"\"\"Resize all labelme annotations (all polygone points and imageWiedth and imageHeight) \n",
    "        in `source_folder` to size `size` and store it in `target_folder`\"\"\"\n",
    "    annotation_files = sorted(gglob(joiner(source_folder, \"*.json\")))\n",
    "    makedirs(target_folder, exist_ok = True)\n",
    "\n",
    "    for annotation_filename in tqdm(annotation_files):\n",
    "        data = open_json(annotation_filename)\n",
    "        filename = Path(annotation_filename).name\n",
    "        target_filename = joiner(target_folder, filename)\n",
    "        \n",
    "        width_factor = data[\"size\"][\"width\"] / size[0]\n",
    "        height_factor = data[\"size\"][\"height\"] / size[1]\n",
    "\n",
    "        data = _resize_annotation_data(data, width_factor, height_factor)\n",
    "        data[\"size\"][\"width\"] = size[0]\n",
    "        data[\"size\"][\"height\"] = size[1]\n",
    "        save_dict(data, target_filename)\n",
    "    print(f\"Resized annotations saved in {target_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering and Normalizing images\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm \n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def filtering_normalizing(input_folder, output_folder):\n",
    "    for image_path in tqdm(glob(os.path.join(input_folder, \"*.jpg\"))):\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "\n",
    "        # Step 1: Noise filtering using Gaussian Blur\n",
    "        denoised_img = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "        # Step 2: Normalize the image to range [0, 1] (min-max normalization)\n",
    "        normalized_img = denoised_img / 255.0\n",
    "\n",
    "        # Step 3: Save the processed image\n",
    "        # Optional: Convert back to uint8 if needed\n",
    "        output_image_path = os.path.join(output_folder, os.path.basename(image_path))\n",
    "        normalized_uint8_img = np.uint8(normalized_img * 255)  # Convert back to [0, 255] range for saving\n",
    "        cv2.imwrite(output_image_path, normalized_uint8_img)\n",
    "\n",
    "    print(f\"Images Processed and saved in {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6935/6935 [08:23<00:00, 13.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized images saved in target folder/train/img\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2010/2010 [02:29<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized images saved in target folder/test/img\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6935/6935 [01:35<00:00, 72.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized annotations saved in target folder/train/ann\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2010/2010 [00:19<00:00, 105.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized annotations saved in target folder/test/ann\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6935/6935 [02:46<00:00, 41.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images Processed and saved in filtered/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2010/2010 [00:47<00:00, 41.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images Processed and saved in filtered/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6935/6935 [05:58<00:00, 19.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved mask for annotation_files in mask/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2010/2010 [01:35<00:00, 20.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved mask for annotation_file in mask/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Paths\n",
    "source_image_folder1 = \"train/img\"  # Original training images folder\n",
    "source_image_folder2 = \"test/img\"  # Original testing images folder\n",
    "resize_image_folder1 = \"target folder/train/img\"   # Resized training images output folder\n",
    "resize_image_folder2 = \"target folder/test/img\"   # Resized testing images output folder\n",
    "source_annotation_folder1 = \"train/ann\"  # Original train annotations (LabelMe JSONs) folder\n",
    "source_annotation_folder2 = \"test/ann\"  # Original test annotations (LabelMe JSONs) folder\n",
    "resize_annotation_folder1 = \"target folder/train/ann\"   # Resized train annotations output folder\n",
    "resize_annotation_folder2 = \"target folder/test/ann\"   # Resized test annotations output folder\n",
    "filtered_image_folder1 = \"filtered/train\" # Filtered and Normalized train images folder\n",
    "filtered_image_folder2 = \"filtered/test\" # Filtered and Normalized test images folder\n",
    "mask_output_folder1 = \"mask/train\"  # Folder where train masks will be saved\n",
    "mask_output_folder2 = \"mask/test\"  # Folder where test masks will be saved\n",
    "\n",
    "# Step 1: Resize images\n",
    "resize_images(source_folder=source_image_folder1, target_folder=resize_image_folder1, size=(512, 512))\n",
    "resize_images(source_folder=source_image_folder2, target_folder=resize_image_folder2, size=(512, 512))\n",
    "\n",
    "# Step 2: Resize annotations\n",
    "resize_annotations(source_folder=source_annotation_folder1, target_folder=resize_annotation_folder1, size=(512, 512))\n",
    "resize_annotations(source_folder=source_annotation_folder2, target_folder=resize_annotation_folder2, size=(512, 512))\n",
    "\n",
    "# Step 3: Filtering and Normalizing images \n",
    "os.makedirs(filtered_image_folder1, exist_ok=True)\n",
    "os.makedirs(filtered_image_folder2, exist_ok=True)\n",
    "filtering_normalizing(resize_image_folder1, filtered_image_folder1)\n",
    "filtering_normalizing(resize_image_folder2, filtered_image_folder2)\n",
    "\n",
    "# Step 3: Convert resized annotations to multi-channel masks\n",
    "os.makedirs(mask_output_folder1, exist_ok=True)  # Ensure the mask output folder exists\n",
    "os.makedirs(mask_output_folder2, exist_ok=True)\n",
    "\n",
    "annotation_files1 = glob(os.path.join(resize_annotation_folder1, \"*.json\"))\n",
    "annotation_files2 = glob(os.path.join(resize_annotation_folder2, \"*.json\"))\n",
    "\n",
    "for annotation_file in tqdm(annotation_files1):\n",
    "    # Convert each resized annotation to a multi-channel mask\n",
    "    annotation_data = open_json(annotation_file)  # Load annotation\n",
    "    mask = labelme2mask(annotation_data)  # Convert to mask\n",
    "\n",
    "    # Save the mask as a NumPy array (.npy) or any other format like PNG\n",
    "    mask_filename = os.path.splitext(os.path.basename(annotation_file))[0] + \"_mask.npy\"\n",
    "    mask_output_path = os.path.join(mask_output_folder1, mask_filename)\n",
    "    \n",
    "    np.save(mask_output_path, mask)  # Save the mask\n",
    "print(f\"Saved mask for annotation_files in {mask_output_folder1}\")\n",
    "    \n",
    "for annotation_file in tqdm(annotation_files2):\n",
    "    # Convert each resized annotation to a multi-channel mask\n",
    "    annotation_data = open_json(annotation_file)  # Load annotation\n",
    "    mask = labelme2mask(annotation_data)  # Convert to mask\n",
    "\n",
    "    # Save the mask as a NumPy array (.npy) or any other format like PNG\n",
    "    mask_filename = os.path.splitext(os.path.basename(annotation_file))[0] + \"_mask.npy\"\n",
    "    mask_output_path = os.path.join(mask_output_folder2, mask_filename)\n",
    "    \n",
    "    np.save(mask_output_path, mask)  # Save the mask\n",
    "print(f\"Saved mask for annotation_file in {mask_output_folder2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import cv2\n",
    "\n",
    "def preprocessed_data_generator(image_folder, mask_folder, batch_size, num_classes=19):\n",
    "    \"\"\"Optimized generator to load preprocessed images and masks directly.\"\"\"\n",
    "    \n",
    "    image_files = sorted(glob(os.path.join(image_folder, \"*.jpg\")))\n",
    "    mask_files = sorted(glob(os.path.join(mask_folder, \"*.npy\")))\n",
    "\n",
    "    # Precompute total steps to avoid recalculating each epoch\n",
    "    num_samples = len(image_files)\n",
    "    if num_samples == 0:\n",
    "        raise ValueError(\"No image files found in the folder\")\n",
    "\n",
    "    print(f\"Found {num_samples} samples.\")\n",
    "\n",
    "    while True:\n",
    "        indices = np.random.permutation(num_samples)  # Shuffle data for each epoch\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_images = []\n",
    "            batch_masks = []\n",
    "            \n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            for idx in batch_indices:\n",
    "                # Load the preprocessed image (check for existence)\n",
    "                image_path = image_files[idx]\n",
    "                mask_path = mask_files[idx]\n",
    "\n",
    "                try:\n",
    "                    image = cv2.imread(image_path)\n",
    "                    mask = np.load(mask_path, allow_pickle=True)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading files: {image_path} or {mask_path} - {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Convert mask to one-hot encoding\n",
    "                mask = to_categorical(mask, num_classes=num_classes)\n",
    "\n",
    "                # Append to batch\n",
    "                batch_images.append(image)\n",
    "                batch_masks.append(mask)\n",
    "\n",
    "            # Ensure batch is not empty\n",
    "            if len(batch_images) > 0 and len(batch_masks) > 0:\n",
    "                # Debug: print batch details to track progress\n",
    "                print(f\"Yielding batch {i//batch_size + 1} with {len(batch_images)} images.\")\n",
    "                yield np.array(batch_images), np.array(batch_masks)\n",
    "\n",
    "            else:\n",
    "                print(f\"Empty batch encountered at batch {i//batch_size + 1}, skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Input\n",
    "\n",
    "# Define the model architecture\n",
    "def create_damage_detection_model(input_shape=(512, 512, 3)):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "\n",
    "    up1 = UpSampling2D(size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(64, (3, 3), activation='relu', padding='same')(up1)\n",
    "\n",
    "    up2 = UpSampling2D(size=(2, 2))(conv4)\n",
    "    conv5 = Conv2D(32, (3, 3), activation='relu', padding='same')(up2)\n",
    "\n",
    "    output = Conv2D(19, (1, 1), activation='softmax')(conv5)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">627</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d (\u001b[38;5;33mUpSampling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m73,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d_1 (\u001b[38;5;33mUpSampling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │        \u001b[38;5;34m18,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m19\u001b[0m)   │           \u001b[38;5;34m627\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">186,131</span> (727.07 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m186,131\u001b[0m (727.07 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">186,131</span> (727.07 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m186,131\u001b[0m (727.07 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = create_damage_detection_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6935 samples.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 722. MiB for an array with shape (4980736, 19) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Train the model using the generator\u001b[39;00m\n\u001b[0;32m     13\u001b[0m steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(train_image_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m))) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[5], line 37\u001b[0m, in \u001b[0;36mpreprocessed_data_generator\u001b[1;34m(image_folder, mask_folder, batch_size, num_classes)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Convert mask to one-hot encoding\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Append to batch\u001b[39;00m\n\u001b[0;32m     40\u001b[0m batch_images\u001b[38;5;241m.\u001b[39mappend(image)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 722. MiB for an array with shape (4980736, 19) and data type float64"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# Define paths to preprocessed images and masks\n",
    "train_image_folder = \"filtered/train\"\n",
    "train_mask_folder = \"mask/train\"\n",
    "\n",
    "# Create the training data generator\n",
    "train_generator = preprocessed_data_generator(train_image_folder, train_mask_folder, 32)\n",
    "\n",
    "# Train the model using the generator\n",
    "steps_per_epoch = len(glob(os.path.join(train_image_folder, \"*.npy\"))) // 32\n",
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
